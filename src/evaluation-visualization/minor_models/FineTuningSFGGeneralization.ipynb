{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_from_disk\n",
    "import pytorch_lightning as pl\n",
    "import sys\n",
    "import os\n",
    "# Add the directory containing lit_sam_model.py to the Python path\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "from model.minor_models.sfgModelST import SumSamModel, SAMDataset3\n",
    "from utils.statistics import calculate_correlation\n",
    "from helperFunctions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Get the path of the script\n",
    "current_file = Path(__file__).resolve() # src/training/your_script.py\n",
    "\n",
    "# 2. Go up one level to 'src', then into 'config'\n",
    "config_path = current_file.parent.parent / \"config\" / \"config_general.yaml\"\n",
    "\n",
    "# 3. Load the YAML\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# 4. Resolve the root of the project (one level above 'src')\n",
    "# This ensures that \"./data\" in the YAML is interpreted relative to the Project_Root\n",
    "PROJECT_ROOT = current_file.parent.parent.parent\n",
    "os.chdir(PROJECT_ROOT) \n",
    "\n",
    "# Extract paths from YAML\n",
    "DATA_DIR = config['paths']['data']\n",
    "CHECKPOINT_DIR = config['paths']['checkpoints']\n",
    "SAM_CHECKPOINT = config['paths']['sam_checkpoint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the test dataset\n",
    "test_dataset = load_from_disk(os.path.join(DATA_DIR, 'datasetTestFinal'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamModel, SamConfig, SamProcessor\n",
    "import torch\n",
    "\n",
    "sam_checkpoint = os.path.join(CHECKPOINT_DIR, \"/sam-adapters-simultaneousencoders-loss-epoch=146-val_loss=0.203-val_iou=0.604.ckpt\")\n",
    "\n",
    "# Create an instance of the model architecture with the loaded configuration\n",
    "#model = LitSamModel(model_name=\"facebook/sam-vit-base\")\n",
    "model = SumSamModel.load_from_checkpoint(sam_checkpoint, model_name=\"sam-vit-b\", normalize = True, adapt = True,)\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(mask1, mask2):\n",
    "\n",
    "    # Ensure the masks are PyTorch tensors\n",
    "    if isinstance(mask1, np.ndarray):\n",
    "        mask1 = torch.tensor(mask1)\n",
    "    if isinstance(mask2, np.ndarray):\n",
    "        mask2 = torch.tensor(mask2)\n",
    "        \n",
    "    # Ensure the masks are binary\n",
    "    mask1 = mask1 > 0\n",
    "    mask2 = mask2 > 0\n",
    "    \n",
    "    # Calculate the intersection and union\n",
    "    intersection = torch.logical_and(mask1, mask2)\n",
    "    union = torch.logical_or(mask1, mask2)\n",
    "    \n",
    "    # Compute the IoU\n",
    "    iou = torch.sum(intersection).float() / torch.sum(union).float()\n",
    "    \n",
    "    return iou, intersection, union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create an instance of the SAMDataset\n",
    "test_dataset_sam = SAMDataset3(dataset=test_dataset, processor=processor, augment=False, target_size=1024, test = True)\n",
    "\n",
    "# Create a DataLoader instance for the validation dataset\n",
    "test_dataloader = DataLoader(test_dataset_sam, batch_size=5, shuffle=False, num_workers=8, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a trainer\n",
    "trainer = pl.Trainer(accelerator='gpu', devices=1)\n",
    "\n",
    "# Run the evaluation\n",
    "trainer.test(model, dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the stored results\n",
    "test_results = model.test_results\n",
    "ground_truth_masks = test_results['ground_truth_masks']\n",
    "predicted_masks = test_results['predicted_masks']\n",
    "individual_ious = test_results['individual_ious']\n",
    "bboxes = test_results['bboxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_masks = np.concatenate(predicted_masks, axis=0)\n",
    "ground_truth_masks = np.concatenate(ground_truth_masks, axis=0)\n",
    "bounding_boxes = np.concatenate(bboxes, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_indices = list(range(384))  # choose first 384 examples for instance\n",
    "test_dataset_subset = test_dataset.select(subset_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_zero_shot = generate_results(test_dataset_subset, ground_truth_masks, predicted_masks, bounding_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the list of dictionaries\n",
    "df_finetune = pd.DataFrame(results_zero_shot)\n",
    "\n",
    "# Save the DataFrame to a file (optional)\n",
    "#df_finetune.to_pickle('dataframe_finetune_20epochs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finetune = pd.read_pickle('dataframe_finetune_20epochs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows with None IoU values\n",
    "df_filtered = df_finetune.dropna(subset=['iou'])\n",
    "\n",
    "\n",
    "# Create a boolean mask for rows with bbox (0,0,512,512) using apply\n",
    "mask_bbox = df_filtered['bbox'].apply(\n",
    "    lambda b: np.all(np.array(b) == np.array((0, 0, 1024, 1024)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered[~mask_bbox]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered[mask_bbox]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iou_statistics(df_filtered, model_name=\"smallprefix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mask_area_vs_iou(df_filtered, model_name='smallprefix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['bbox'] = df_filtered['bbox'].apply(lambda b: np.squeeze(b) if np.array(b).ndim == 2 else b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iou_vs_area_ratio(df_filtered, model_name='smallprefix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iou_vs_num_avalanches(df_filtered, model_name='smallprefix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iou_for_mask_area(df_filtered, model_name='smallprefix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_mask_area_iou_correlation(df_filtered, calculate_correlation, model_name='smallprefix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_mask_area_iou_correlation(df_filtered, calculate_correlation, model_name='smallprefix', scale='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_num_avalanche_iou_correlation(df_filtered, calculate_correlation, model_name='smallprefix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_area_ratio_iou_correlation(df_filtered, calculate_correlation, model_name='smallprefix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_area_ratio_iou_correlation(df_filtered, calculate_correlation, model_name='smallprefix', scale='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helperFunctions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_zero_shot = compute_error_percentages(results_zero_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove exemples with bounding boxes (0,0,512,512)\n",
    "results_zero_shot_copy = results_zero_shot.copy()\n",
    "results_zero_shot = [res for res in results_zero_shot if not (res['bbox'] == [0, 0, 1024, 1024]).all()]\n",
    "results_zero_shot_general = [res for res in results_zero_shot_copy if (res['bbox'] == [0, 0, 1024, 1024]).all()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_zero_shot = results_zero_shot_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_false_pos_neg_percentages(results_zero_shot,treshold= 0.3, model_name = \"smallprefix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_false_pos_neg_percentages(results_zero_shot_general,treshold= 0.3, model_name = \"smallprefix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New cell: Visualization with mask background and bbox outlines\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "num_results_to_display = 15\n",
    "\n",
    "for idx, res in enumerate(results_zero_shot[:num_results_to_display]):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # --- Left subplot: Ground truth mask with false negative outlines ---\n",
    "    mask = res['mask']\n",
    "    if mask.ndim == 2:\n",
    "        gt_img = (mask * 255).astype(np.uint8)\n",
    "        gt_img = cv2.cvtColor(gt_img, cv2.COLOR_GRAY2RGB)\n",
    "    else:\n",
    "        gt_img = mask.copy()\n",
    "    ax1.imshow(gt_img)\n",
    "    ax1.set_title(\"Ground Truth with False Negatives\")\n",
    "    \n",
    "    false_negatives = res.get('false_negatives', [])\n",
    "    print(\"Lenght of false negatives:\", len(false_negatives))\n",
    "    for bbox in false_negatives:\n",
    "        # bbox is expected as [x_min, y_min, x_max, y_max]\n",
    "        rect = patches.Rectangle((bbox[0], bbox[1]),\n",
    "                                 bbox[2] - bbox[0],\n",
    "                                 bbox[3] - bbox[1],\n",
    "                                 linewidth=2,\n",
    "                                 edgecolor='r',\n",
    "                                 facecolor='none')\n",
    "        ax1.add_patch(rect)\n",
    "        \n",
    "    \n",
    "    # --- Right subplot: Predicted mask with false positive outlines ---\n",
    "    pred_mask = res['calculated_mask']\n",
    "    if pred_mask.ndim == 2:\n",
    "        pred_img = (pred_mask * 255).astype(np.uint8)\n",
    "        pred_img = cv2.cvtColor(pred_img, cv2.COLOR_GRAY2RGB)\n",
    "    else:\n",
    "        pred_img = pred_mask.copy()\n",
    "    ax2.imshow(pred_img)\n",
    "    ax2.set_title(\"Prediction with False Positives\")\n",
    "    \n",
    "    false_positives = res.get('false_positives', [])\n",
    "    print(\"Lenght of false positives:\", len(false_positives))\n",
    "    for bbox in false_positives:\n",
    "        rect = patches.Rectangle((bbox[0], bbox[1]),\n",
    "                                 bbox[2] - bbox[0],\n",
    "                                 bbox[3] - bbox[1],\n",
    "                                 linewidth=2,\n",
    "                                 edgecolor='b',\n",
    "                                 facecolor='none')\n",
    "        ax2.add_patch(rect)\n",
    "        \n",
    "\n",
    "    ax1.axis('off')\n",
    "    ax2.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New cell: Visualization of samples with high error percentages\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define a threshold for high error (e.g., 30% error)\n",
    "threshold = 0.8\n",
    "\n",
    "# Filter samples with either false negative or false positive percentage above the threshold\n",
    "high_error_results = [\n",
    "    res for res in results_zero_shot \n",
    "    if res.get('percentage_false_negatives', 0) > threshold or res.get('percentage_false_positives', 0) > threshold\n",
    "]\n",
    "\n",
    "print(f\"Number of high error samples: {len(high_error_results)}\")\n",
    "\n",
    "for idx, res in enumerate(high_error_results):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # --- Left: Ground truth mask with false negative outlines --- \n",
    "    mask = res['mask']\n",
    "    if mask.ndim == 2:\n",
    "        gt_img = (mask * 255).astype(np.uint8)\n",
    "        gt_img = cv2.cvtColor(gt_img, cv2.COLOR_GRAY2RGB)\n",
    "    else:\n",
    "        gt_img = mask.copy()\n",
    "    ax1.imshow(gt_img)\n",
    "    ax1.set_title(\"Ground Truth with False Negatives\")\n",
    "    \n",
    "    for bbox in res.get('false_negatives', []):\n",
    "        rect = patches.Rectangle((bbox[0], bbox[1]),\n",
    "                                 bbox[2] - bbox[0],\n",
    "                                 bbox[3] - bbox[1],\n",
    "                                 linewidth=2,\n",
    "                                 edgecolor='r',\n",
    "                                 facecolor='none')\n",
    "        ax1.add_patch(rect)\n",
    "        \n",
    "    # --- Right: Predicted mask with false positive outlines ---\n",
    "    pred_mask = res['calculated_mask']\n",
    "    if pred_mask.ndim == 2:\n",
    "        pred_img = (pred_mask * 255).astype(np.uint8)\n",
    "        pred_img = cv2.cvtColor(pred_img, cv2.COLOR_GRAY2RGB)\n",
    "    else:\n",
    "        pred_img = pred_mask.copy()\n",
    "    ax2.imshow(pred_img)\n",
    "    ax2.set_title(\"Prediction with False Positives\")\n",
    "    \n",
    "    for bbox in res.get('false_positives', []):\n",
    "        rect = patches.Rectangle((bbox[0], bbox[1]),\n",
    "                                 bbox[2] - bbox[0],\n",
    "                                 bbox[3] - bbox[1],\n",
    "                                 linewidth=2,\n",
    "                                 edgecolor='b',\n",
    "                                 facecolor='none')\n",
    "        ax2.add_patch(rect)\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "    if idx >= 10:  # Limit to first 5 high error samples for visualization\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
