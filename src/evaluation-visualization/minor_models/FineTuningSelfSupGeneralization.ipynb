{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_from_disk, concatenate_datasets\n",
    "import pytorch_lightning as pl\n",
    "import sys\n",
    "import os\n",
    "# Add the directory containing lit_sam_model.py to the Python path\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "from model.minor_models.selfSupervisedModel import selfSupSamModel, SAMDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Get the path of the script\n",
    "current_file = Path(__file__).resolve() # src/training/your_script.py\n",
    "\n",
    "# 2. Go up one level to 'src', then into 'config'\n",
    "config_path = current_file.parent.parent / \"config\" / \"config_general.yaml\"\n",
    "\n",
    "# 3. Load the YAML\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# 4. Resolve the root of the project (one level above 'src')\n",
    "# This ensures that \"./data\" in the YAML is interpreted relative to the Project_Root\n",
    "PROJECT_ROOT = current_file.parent.parent.parent\n",
    "os.chdir(PROJECT_ROOT) \n",
    "\n",
    "# Extract paths from YAML\n",
    "DATA_DIR = config['paths']['data']\n",
    "CHECKPOINT_DIR = config['paths']['checkpoints']\n",
    "SAM_CHECKPOINT = config['paths']['sam_checkpoint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the test dataset\n",
    "test_dataset = load_from_disk(os.path.join(DATA_DIR, \"/home/gelato/Avalanche-Segmentation-with-Sam/code/dataprocessing/datasetTestSelfSupSlope\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the training datasets\n",
    "test_dataset = concatenate_datasets([test_dataset1, test_dataset2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamModel, SamConfig, SamProcessor\n",
    "import torch\n",
    "\n",
    "sam_checkpoint = \"/home/gelato/Avalanche-Segmentation-with-Sam/code/training/checkpointsSelfSup/sam-float-selfsup_slope-model-epoch=146-val_loss=0.0013.ckpt\"\n",
    "sam_checkpoint = os.path.join(CHECKPOINT_DIR, \"/sam-float-selfsup_slope-model-epoch=146-val_loss=0.0013.ckpt\")\n",
    "\n",
    "# Create an instance of the model architecture with the loaded configuration\n",
    "model = selfSupSamModel.load_from_checkpoint(sam_checkpoint, model_name=\"facebook/sam-vit-base\", normalize = True)\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "# set the device to cuda if available, otherwise use cpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "image = np.array(test_dataset[idx][\"image\"])\n",
    "print(image.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max:\" + str(np.max(image)))\n",
    "print(\"Min:\" + str(np.min(image)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = test_dataset[idx][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4\n",
    "image = np.array(test_dataset[idx][\"image\"], np.float32)\n",
    "mask = test_dataset[idx]['label']\n",
    "mask = np.array(mask, np.float32)\n",
    "\n",
    "image_copy = image.copy()\n",
    "mask_copy = mask.copy()\n",
    "\n",
    "# Draw the bounding boxes on the copied image\n",
    "for (x, y, w, h) in test_dataset[idx:idx+1]['box']:\n",
    "    cv2.rectangle(image_copy, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Green box with thickness 2\n",
    "    cv2.rectangle(mask_copy, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Green box with thickness 2\n",
    "\n",
    "# Convert the copied images from BGR to RGB (OpenCV uses BGR by default)\n",
    "image_copy_rgb = cv2.cvtColor(image_copy, cv2.COLOR_RGB2RGBA)\n",
    "mask_copy_rgb = cv2.cvtColor(mask_copy, cv2.COLOR_RGB2RGBA)\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_RGB2RGBA)\n",
    "\n",
    "# Display the original image, mask, and image with bounding boxes using matplotlib\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].imshow(image_rgb)\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(mask_copy_rgb)\n",
    "axes[1].set_title('Mask with Bounding Boxes')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(image_copy_rgb)\n",
    "axes[2].set_title('Image with Bounding Boxes')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create an instance of the SAMDataset\n",
    "test_dataset_sam = SAMDataset(dataset=test_dataset, processor=processor, augment=False)\n",
    "\n",
    "# Create a DataLoader instance for the validation dataset\n",
    "test_dataloader = DataLoader(test_dataset_sam, batch_size=5, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(test_dataloader))\n",
    "\n",
    "# Get the first image, mask, and boxes from the batch\n",
    "image = batch[\"pixel_values\"][1]\n",
    "mask = batch[\"ground_truth_mask\"][1]\n",
    "\n",
    "print(image.shape)\n",
    "print(mask.shape)\n",
    "print(image.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mask.min(), mask.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "def display_image_mask_boxes(image, mask):\n",
    "    # Convert the image and mask to PIL images\n",
    "    image = image.permute(1, 2, 0).cpu().numpy()\n",
    "    mask = mask.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Display the image\n",
    "    ax[0].imshow(image)\n",
    "    ax[0].set_title(\"Image\")\n",
    "    ax[0].axis(\"off\")\n",
    "\n",
    "    # Display the mask\n",
    "    ax[1].imshow(mask)\n",
    "    ax[1].set_title(\"Mask\")\n",
    "    ax[1].axis(\"off\")\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Display the image, mask, and bounding boxes\n",
    "display_image_mask_boxes(image, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the dataloader and display different batches\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "    if i >= 5:  # Display 5 different batches\n",
    "        break\n",
    "\n",
    "    # Get the first image, mask, and boxes from the batch\n",
    "    image = batch[\"pixel_values\"][0]\n",
    "    mask = batch[\"ground_truth_mask\"][0]\n",
    "\n",
    "    # Display the image, mask, and bounding boxes\n",
    "    display_image_mask_boxes(image, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def denormalize(image, pixel_mean, pixel_std):\n",
    "    \"\"\"\n",
    "    Denormalizes an image tensor.\n",
    "    \n",
    "    Args:\n",
    "        image (torch.Tensor): Tensor of shape (C, H, W) in normalized space.\n",
    "        pixel_mean (torch.Tensor): Tensor of shape (C, 1, 1).\n",
    "        pixel_std (torch.Tensor): Tensor of shape (C, 1, 1).\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Denormalized image, clamped between 0 and 1.\n",
    "    \"\"\"\n",
    "    image = image * pixel_std + pixel_mean\n",
    "    return torch.clamp(image, 0, 1)\n",
    "\n",
    "def display_prediction(image_norm, prediction, ground_truth, model):\n",
    "    \"\"\"\n",
    "    Displays the denormalized input image and the model's prediction.\n",
    "    \n",
    "    Args:\n",
    "        image_norm (torch.Tensor): Input image tensor (C, H, W) after normalization.\n",
    "        prediction (torch.Tensor): Model output (usually in same value range as the input).\n",
    "        ground_truth (torch.Tensor): Ground truth mask.\n",
    "        model: The Lightning module (or its inner model) that contains the pixel_mean and pixel_std buffers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert tensors (C, H, W) to numpy arrays (H, W, C)\n",
    "    img_np = image_norm.permute(1, 2, 0).cpu().numpy()\n",
    "    gt_np = ground_truth.permute(1, 2, 0).cpu().numpy()\n",
    "    # For prediction, if it is not in a displayable range, you might need to normalize it.\n",
    "    # Here, we assume the prediction is either single-channel (mask) or RGB.\n",
    "    pred_np = prediction.squeeze(0).cpu().numpy()\n",
    "    if pred_np.ndim == 2:\n",
    "        cmap = \"gray\"\n",
    "    else:\n",
    "        pred_np = pred_np.transpose(1, 2, 0)  # from (C,H,W) to (H,W,C)\n",
    "        cmap = None\n",
    "\n",
    "    # Decide number of subplots based on whether last_mask exists\n",
    "    if hasattr(model.model.image_encoder, \"last_mask\") and model.model.image_encoder.last_mask is not None:\n",
    "        ncols = 4\n",
    "    else:\n",
    "        ncols = 3\n",
    "\n",
    "    # Create a figure with three subplots\n",
    "    fig, axs = plt.subplots(1, ncols, figsize=(6 * ncols, 6))\n",
    "    axs[0].imshow(img_np)\n",
    "    axs[0].set_title(\"Input Image\")\n",
    "    axs[0].axis(\"off\")\n",
    "    \n",
    "    axs[1].imshow(gt_np)\n",
    "    axs[1].set_title(\"Ground Truth Mask\")\n",
    "    axs[1].axis(\"off\")\n",
    "    \n",
    "    axs[2].imshow(pred_np, cmap=cmap)\n",
    "    axs[2].set_title(\"Prediction\")\n",
    "    axs[2].axis(\"off\")\n",
    "\n",
    "    # Display the self-supervised mask if it exists\n",
    "    if ncols == 4:\n",
    "        mask_np = model.model.image_encoder.last_mask.cpu().numpy()\n",
    "        axs[3].imshow(mask_np, cmap=\"gray\")\n",
    "        axs[3].set_title(\"SelfSup Mask\")\n",
    "        axs[3].axis(\"off\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the dataloader and display different batches\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "    if i >= 5:  # Display 5 different batches\n",
    "        break\n",
    "    # Assume 'model' is your selfSupSamModel instance and you have a sample batch.\n",
    "    # Get one sample image and its prediction.\n",
    "    image_norm = batch[\"pixel_values\"][0]  # normalized input\n",
    "    gt_mask = batch[\"ground_truth_mask\"][0]  # ground truth mask\n",
    "    with torch.no_grad():\n",
    "        pred = model(batch[\"pixel_values\"].to(model.device))\n",
    "        # Resize prediction if needed:\n",
    "        pred = torch.nn.functional.interpolate(pred, size=image_norm.shape[-2:], mode='bilinear', align_corners=False)\n",
    "    pred = pred[0]  # take first sample\n",
    "\n",
    "    # Display the denormalized image and prediction.\n",
    "    display_prediction(image_norm, pred, gt_mask, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def analyze_mask(mask):\n",
    "    \"\"\"\n",
    "    Analyzes a binary mask and returns the counts and percentages of masked (white) and unmasked (black) pixels.\n",
    "    \n",
    "    Args:\n",
    "        mask (np.ndarray): A 2D binary mask (0 for unmasked, 1 for masked).\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing count and percentage for masked and unmasked pixels.\n",
    "    \"\"\"\n",
    "    total_pixels = mask.size\n",
    "    masked_pixels = np.sum(mask > 0)\n",
    "    unmasked_pixels = total_pixels - masked_pixels\n",
    "    perc_masked = (masked_pixels / total_pixels) * 100\n",
    "    perc_unmasked = (unmasked_pixels / total_pixels) * 100\n",
    "    \n",
    "    return {\n",
    "        \"masked_pixels\": masked_pixels,\n",
    "        \"unmasked_pixels\": unmasked_pixels,\n",
    "        \"perc_masked\": perc_masked,\n",
    "        \"perc_unmasked\": perc_unmasked\n",
    "    }\n",
    "\n",
    "# Create a random mask of size 64x64 that masks ~30% of the pixels.\n",
    "mask = (np.random.rand(64, 64) < 0.3).astype(np.uint8)\n",
    "\n",
    "# Analyze the mask:\n",
    "results = analyze_mask(mask)\n",
    "print(\"Masked pixels:\", results[\"masked_pixels\"])\n",
    "print(\"Unmasked pixels:\", results[\"unmasked_pixels\"])\n",
    "print(\"Percentage masked: {:.2f}%\".format(results[\"perc_masked\"]))\n",
    "print(\"Percentage unmasked: {:.2f}%\".format(results[\"perc_unmasked\"]))\n",
    "\n",
    "# Optionally, visualize the mask:\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(mask, cmap=\"gray\")\n",
    "plt.title(\"Random Mask (approx. 30% masked)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a trainer\n",
    "trainer = pl.Trainer(accelerator='gpu', devices=1)\n",
    "\n",
    "# Run the evaluation\n",
    "trainer.test(model, dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.test_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[4]['test_iou'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the stored results\n",
    "test_results = model.test_results\n",
    "ground_truth_masks = test_results['ground_truth_masks']\n",
    "predicted_masks = test_results['predicted_masks']\n",
    "individual_ious = test_results['individual_ious']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_seg = ground_truth_masks[2]\n",
    "print(sam_seg)\n",
    "print(sam_seg.__contains__(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_masks = np.concatenate(predicted_masks, axis=0)\n",
    "ground_truth_masks = np.concatenate(ground_truth_masks, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_zero_shot = []\n",
    "\n",
    "for index in range(len(test_dataset)):\n",
    "        mask = ground_truth_masks[index]\n",
    "        sam_seg = predicted_masks[index]\n",
    "\n",
    "        sam_seg_prob = torch.sigmoid(torch.tensor(sam_seg))\n",
    "        # convert soft mask to hard mask\n",
    "        sam_seg_prob = sam_seg_prob.cpu().numpy().squeeze()\n",
    "        sam_seg = (sam_seg_prob > 0.5).astype(np.uint8)\n",
    "\n",
    "        # Calculate IoU\n",
    "        iou, intersection, union = calculate_iou(mask, sam_seg)\n",
    "\n",
    "        results_zero_shot.append({'mask': mask,\n",
    "                                  'calculated_mask': sam_seg, \n",
    "                                  'intersection': intersection.cpu().squeeze().numpy(), \n",
    "                                  'union': union.cpu().squeeze().numpy(), \n",
    "                                  'iou': iou.cpu().numpy(),\n",
    "                                  'empty': False, \n",
    "                                  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the list of dictionaries\n",
    "df_finetune = pd.DataFrame(results_zero_shot)\n",
    "\n",
    "# Save the DataFrame to a file (optional)\n",
    "#df_finetune.to_pickle('dataframe_finetune_20epochs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finetune = pd.read_pickle('dataframe_finetune_20epochs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finetune.at[counter, 'iou'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = counter\n",
    "for index in range(len(df_finetune)):\n",
    "    if df_finetune.at[index, 'mask_area'].item() > 5000:\n",
    "        if(count == 0):\n",
    "            count = index\n",
    "            break\n",
    "        else:\n",
    "            count -= 1\n",
    "counter += 1\n",
    "\n",
    "mask1_np = df_finetune.at[count, 'mask']\n",
    "combined_mask_np = df_finetune.at[count, 'calculated_mask']\n",
    "intersection_np = df_finetune.at[count, 'intersection']\n",
    "union_np = df_finetune.at[count, 'union']\n",
    "\n",
    "# Create a visualization\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "# Original mask1\n",
    "axes[0].imshow(mask1_np, cmap='gray')\n",
    "axes[0].set_title('Mask')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Combined mask\n",
    "axes[1].imshow(combined_mask_np, cmap='gray')\n",
    "axes[1].set_title('Calculated Mask')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Intersection\n",
    "axes[2].imshow(intersection_np, cmap='gray')\n",
    "axes[2].set_title('Intersection')\n",
    "axes[2].axis('off')\n",
    "\n",
    "# Union\n",
    "axes[3].imshow(union_np, cmap='gray')\n",
    "axes[3].set_title('Union')\n",
    "axes[3].axis('off')\n",
    "\n",
    "# Add IoU text to the plot\n",
    "fig.suptitle(f'IoU: {df_finetune.at[count, 'iou']:.4f}', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_finetune.at[count, 'mask_area'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows with None IoU values\n",
    "df_filtered = df_finetune.dropna(subset=['iou'])\n",
    "\n",
    "# Extract IoU values from the filtered DataFrame\n",
    "iou_values = df_filtered['iou'].tolist()\n",
    "#iou_values = individual_ious\n",
    "\n",
    "# Calculate the average IoU\n",
    "average_iou = sum(iou_values) / len(iou_values)\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(iou_values)), iou_values, color='blue')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('IoU')\n",
    "plt.title('IoU Values for Different Samples')\n",
    "plt.text(0.5, 0.95, f'Average IoU: {average_iou:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.show()\n",
    "\n",
    "# Create a line plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(iou_values)), iou_values, marker='o', linestyle='-', color='blue')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('IoU')\n",
    "plt.title('IoU Values for Different Samples')\n",
    "plt.text(0.5, 0.95, f'Average IoU: {average_iou:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows with None IoU values\n",
    "df_filtered = df_finetune.dropna(subset=['iou'])\n",
    "\n",
    "# Extract IoU values from the filtered DataFrame\n",
    "iou_values = df_filtered['iou'].tolist()\n",
    "#iou_values = individual_ious\n",
    "\n",
    "# Calculate the average IoU\n",
    "average_iou = sum(iou_values) / len(iou_values)\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(iou_values)), iou_values, color='blue')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('IoU')\n",
    "plt.title('IoU Values for Different Samples (only gaussian noise)')\n",
    "plt.text(0.5, 0.95, f'Average IoU: {average_iou:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.show()\n",
    "\n",
    "# Create a line plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(iou_values)), iou_values, marker='o', linestyle='-', color='blue')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('IoU')\n",
    "plt.title('IoU Values for Different Samples (only gaussian noise)')\n",
    "plt.text(0.5, 0.95, f'Average IoU: {average_iou:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the area of each mask\n",
    "df_finetune['mask_area'] = df_finetune['mask'].apply(lambda mask: np.sum(mask))\n",
    "\n",
    "# Filter out rows where the mask area is above 5000\n",
    "df_filtered2 = df_finetune[df_finetune['mask_area'] <= 500]\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_filtered2['mask_area'], df_filtered2['iou'], marker='o', color='blue')\n",
    "plt.xlabel('Mask Area')\n",
    "plt.ylabel('IoU')\n",
    "plt.title('IoU vs. Mask Area')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a scatter plot with a logarithmic scale for the mask area axis\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_finetune['mask_area'], df_finetune['iou'], marker='o', color='blue')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Mask Area (log scale)')\n",
    "plt.ylabel('IoU')\n",
    "plt.title('IoU vs. Mask Area')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
