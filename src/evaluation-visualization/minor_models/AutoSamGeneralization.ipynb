{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_from_disk\n",
    "import pytorch_lightning as pl\n",
    "import sys\n",
    "import os\n",
    "# Add the directory containing lit_sam_model.py to the Python path\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "from model.minor_models.autoSamModel import LitSamModel\n",
    "from model.samDataset import SAMDataset, SAMDataset3\n",
    "from utils.statistics import calculate_correlation\n",
    "from helperFunctions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Get the path of the script\n",
    "current_file = Path(__file__).resolve() # src/training/your_script.py\n",
    "\n",
    "# 2. Go up one level to 'src', then into 'config'\n",
    "config_path = current_file.parent.parent / \"config\" / \"config_general.yaml\"\n",
    "\n",
    "# 3. Load the YAML\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# 4. Resolve the root of the project (one level above 'src')\n",
    "# This ensures that \"./data\" in the YAML is interpreted relative to the Project_Root\n",
    "PROJECT_ROOT = current_file.parent.parent.parent\n",
    "os.chdir(PROJECT_ROOT) \n",
    "\n",
    "# Extract paths from YAML\n",
    "DATA_DIR = config['paths']['data']\n",
    "CHECKPOINT_DIR = config['paths']['checkpoints']\n",
    "SAM_CHECKPOINT = config['paths']['sam_checkpoint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the test dataset\n",
    "\n",
    "test_dataset = load_from_disk(os.path.join(DATA_DIR, testDatasetFinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamModel, SamConfig, SamProcessor\n",
    "import torch\n",
    "\n",
    "sam_checkpoint = os.path.join(CHECKPOINT_DIR, \"/autosam/sam-auto-bestbase-smallnet-model-epoch=105-val_loss=0.2268-val_iou=0.590.ckpt\")\n",
    "\n",
    "# Create an instance of the model architecture with the loaded configuration\n",
    "model = LitSamModel.load_from_checkpoint(sam_checkpoint, model_name=\"vit-b\", normalize = True, adapt = True)\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "# set the device to cuda if available, otherwise use cpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(mask1, mask2):\n",
    "\n",
    "    # Ensure the masks are PyTorch tensors\n",
    "    if isinstance(mask1, np.ndarray):\n",
    "        mask1 = torch.tensor(mask1)\n",
    "    if isinstance(mask2, np.ndarray):\n",
    "        mask2 = torch.tensor(mask2)\n",
    "        \n",
    "    # Ensure the masks are binary\n",
    "    mask1 = mask1 > 0\n",
    "    mask2 = mask2 > 0\n",
    "    \n",
    "    # Calculate the intersection and union\n",
    "    intersection = torch.logical_and(mask1, mask2)\n",
    "    union = torch.logical_or(mask1, mask2)\n",
    "    \n",
    "    # Compute the IoU\n",
    "    iou = torch.sum(intersection).float() / torch.sum(union).float()\n",
    "    \n",
    "    return iou, intersection, union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create an instance of the SAMDataset\n",
    "test_dataset_sam = SAMDataset3(dataset=test_dataset, processor=processor, augment=False, test = True)\n",
    "\n",
    "# Create a DataLoader instance for the validation dataset\n",
    "test_dataloader = DataLoader(test_dataset_sam, batch_size=5, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a trainer\n",
    "trainer = pl.Trainer(accelerator='gpu', devices=1)\n",
    "\n",
    "# Run the evaluation\n",
    "trainer.test(model, dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the stored results\n",
    "test_results = model.test_results\n",
    "ground_truth_masks = test_results['ground_truth_masks']\n",
    "predicted_masks = test_results['predicted_masks']\n",
    "individual_ious = test_results['individual_ious']\n",
    "bboxes = test_results['bboxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_masks = np.concatenate(predicted_masks, axis=0)\n",
    "ground_truth_masks = np.concatenate(ground_truth_masks, axis=0)\n",
    "bounding_boxes = np.concatenate(bboxes, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_zero_shot = generate_results(test_dataset, ground_truth_masks, predicted_masks, bounding_boxes, calculate_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove exemples with bounding boxes (0,0,512,512)\n",
    "results_zero_shot_copy = results_zero_shot.copy()\n",
    "results_zero_shot = [res for res in results_zero_shot if not (res['bbox'] == [0, 0, 1024, 1024]).all()]\n",
    "results_zero_shot_general = [res for res in results_zero_shot_copy if (res['bbox'] == [0, 0, 1024, 1024]).all()]\n",
    "\n",
    "results, metrics =calculate_pixel_based_metrics(results_zero_shot)\n",
    "\n",
    "print(metrics)\n",
    "\n",
    "results, metrics =calculate_pixel_based_metrics(results_zero_shot_general)\n",
    "\n",
    "print(metrics)\n",
    "\n",
    "results_zero_shot = results_zero_shot_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the list of dictionaries\n",
    "df_finetune = pd.DataFrame(results_zero_shot)\n",
    "\n",
    "# Save the DataFrame to a file (optional)\n",
    "#df_finetune.to_pickle('dataframe_name.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows with None IoU values\n",
    "df_filtered = df_finetune.dropna(subset=['iou'])\n",
    "\n",
    "\n",
    "# Create a boolean mask for rows with bbox (0,0,512,512) using apply\n",
    "mask_bbox = df_filtered['bbox'].apply(\n",
    "    lambda b: np.all(np.array(b) == np.array((0, 0, 1024, 1024)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to exclude rows with bbox (0,0,512,512)\n",
    "\n",
    "df_filtered = df_filtered[~mask_bbox]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "df_filtered = df_filtered[mask_bbox]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iou_statistics(df_filtered, model_name='AutoSAM Best Base with SmallNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mask_area_vs_iou(df_filtered, model_name='AutoSAM SAM Base with Resnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the area of each mask\n",
    "df_filtered['mask_area'] = df_filtered['mask'].apply(lambda mask: np.sum(mask))\n",
    "\n",
    "# Filter out rows where the mask area is above 5000\n",
    "df_filtered2 = df_filtered[df_filtered['mask_area'] <= 1000]\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_filtered2['mask_area'], df_filtered2['iou'], marker='o', color='blue')\n",
    "plt.xlabel('Mask Area')\n",
    "plt.ylabel('IoU')\n",
    "plt.title('IoU vs. Mask Area')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['bbox'] = df_filtered['bbox'].apply(lambda b: np.squeeze(b) if np.array(b).ndim == 2 else b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a scatter plot with a logarithmic scale for the mask area axis\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_filtered['mask_area'], df_filtered['iou'], marker='o', color='blue')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Mask Area (log scale)')\n",
    "plt.ylabel('IoU')\n",
    "plt.title('IoU vs. Mask Area')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iou_vs_area_ratio(df_filtered, model_name='AutoSAM SAM Base with Resnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iou_vs_num_avalanches(df_filtered, model_name='AutoSAM multidimnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iou_for_mask_area(df_filtered, model_name='AutoSAM SAM Base with Resnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['mask_area'] = df_filtered['mask'].apply(lambda mask: np.sum(np.array(mask, dtype=np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_mask_area_iou_correlation(df_filtered, calculate_correlation, model_name='AutoSAM multidimnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure no zeros (to avoid log issues) by adding a small constant if needed.\n",
    "mask_area_numeric = np.asarray(df_filtered['mask_area'].values, dtype=np.float32)\n",
    "iou_numeric = np.asarray(df_filtered['iou'].values, dtype=np.float32)\n",
    "epsilon = 1e-8\n",
    "mask_area_log = np.log(mask_area_numeric + epsilon)\n",
    "\n",
    "corr, p_value = calculate_correlation(mask_area_log, iou_numeric)\n",
    "print(f\"Correlation on log-transformed values: {corr}, p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_num_avalanche_iou_correlation(df_filtered, calculate_correlation, model_name='AutoSAM multidimnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_area_ratio_iou_correlation(df_filtered, calculate_correlation, model_name='AutoSAM multidimnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_area_ratio_iou_correlation(df_filtered, calculate_correlation, model_name='AutoSAM multidimnet', scale='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a file (optional)\n",
    "df_filtered.to_pickle('dataframe_bestadaptersmodel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finetune = df_finetune = pd.read_pickle('dataframe_bestadaptersmodel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find bounding boxes for each group of disconnected white pixels\n",
    "def find_bounding_boxes(mask):\n",
    "    # Ensure the mask is an 8-bit image.\n",
    "    if mask.dtype != \"uint8\":\n",
    "        # If mask values are in range 0-1, scale them by 255\n",
    "        if mask.max() <= 1:\n",
    "            mask_uint8 = (mask * 255).astype('uint8')\n",
    "        else:\n",
    "            mask_uint8 = mask.astype('uint8')\n",
    "    else:\n",
    "        mask_uint8 = mask\n",
    "\n",
    "    # Find contours in the binary mask\n",
    "    contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Compute bounding boxes for each contour and convert (x, y, w, h) to (x_min, y_min, x_max, y_max)\n",
    "    bounding_boxes = []\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        bounding_boxes.append([x, y, x + w, y + h])\n",
    "    return bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for res in results_zero_shot:\n",
    "    false_positives = []\n",
    "    false_negatives = []\n",
    "    # The predicted and ground truth avalanche masks\n",
    "    pred_mask = res['calculated_mask']\n",
    "    true_mask = res['mask']\n",
    "\n",
    "    original_bbox = find_bounding_boxes(true_mask)\n",
    "    pred_bbox = find_bounding_boxes(pred_mask)\n",
    "\n",
    "    for bbox in original_bbox:\n",
    "        predicted = False\n",
    "        for pred in pred_bbox:\n",
    "\n",
    "            # Assuming each bbox is [x_min, y_min, x_max, y_max]\n",
    "            x_left = max(bbox[0], pred[0])\n",
    "            y_top = max(bbox[1], pred[1])\n",
    "            x_right = min(bbox[2], pred[2])\n",
    "            y_bottom = min(bbox[3], pred[3])\n",
    "\n",
    "            # Calculate the overlap area\n",
    "            if x_right < x_left or y_bottom < y_top:\n",
    "                overlap_area = 0\n",
    "            else:\n",
    "                overlap_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "\n",
    "            overlap = overlap_area / ((bbox[2] - bbox[0]) * (bbox[3] - bbox[1]) )#+ (pred[2] - pred[0]) * (pred[3] - pred[1]) - overlap_area)\n",
    "            if overlap > 0.3:\n",
    "                predicted = True\n",
    "                break\n",
    "        if not predicted:\n",
    "            false_negatives.append(bbox)\n",
    "    for bbox in pred_bbox:\n",
    "        original = False\n",
    "        for originalbbox in original_bbox:\n",
    "            # Assuming each bbox is [x_min, y_min, x_max, y_max]\n",
    "            x_left = max(bbox[0], originalbbox[0])\n",
    "            y_top = max(bbox[1], originalbbox[1])\n",
    "            x_right = min(bbox[2], originalbbox[2])\n",
    "            y_bottom = min(bbox[3], originalbbox[3])\n",
    "\n",
    "            # Calculate the overlap area\n",
    "            if x_right < x_left or y_bottom < y_top:\n",
    "                overlap_area = 0\n",
    "            else:\n",
    "                overlap_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "\n",
    "            overlap = overlap_area / ((bbox[2] - bbox[0]) * (bbox[3] - bbox[1]) )#+ (originalbbox[2] - originalbbox[0]) * (originalbbox[3] - originalbbox[1]) - overlap_area)\n",
    "            if overlap > 0.3:\n",
    "                original = True\n",
    "                break\n",
    "        if not original:\n",
    "            false_positives.append(bbox)\n",
    "            \n",
    "    # Add the results into the current dictionary element\n",
    "    res['false_negatives'] = false_negatives\n",
    "    res['false_positives'] = false_positives\n",
    "    res['percentage_false_negatives'] = len(false_negatives) / len(original_bbox) if len(original_bbox) > 0 else 0\n",
    "    res['percentage_false_positives'] = len(false_positives) / len(pred_bbox) if len(pred_bbox) > 0 else 0\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_zero_shot = compute_error_percentages(results_zero_shot, threshold=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove exemples with bounding boxes (0,0,512,512)\n",
    "results_zero_shot_copy = results_zero_shot.copy()\n",
    "results_zero_shot = [res for res in results_zero_shot if not (res['bbox'] == [0, 0, 1024, 1024]).all()]\n",
    "results_zero_shot_general = [res for res in results_zero_shot_copy if (res['bbox'] == [0, 0, 1024, 1024]).all()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_zero_shot = results_zero_shot_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New cell: Plot the percentage of false positives and false negatives with averages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract percentages from the results\n",
    "false_negatives_pct = [res.get('percentage_false_negatives', 0) for res in results_zero_shot]\n",
    "false_positives_pct = [res.get('percentage_false_positives', 0) for res in results_zero_shot]\n",
    "indices = np.arange(len(results_zero_shot))\n",
    "\n",
    "# Compute average percentages\n",
    "avg_false_negatives = np.mean(false_negatives_pct)\n",
    "avg_false_positives = np.mean(false_positives_pct)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(indices - 0.15, false_negatives_pct, width=0.3, color='red', label='False Negatives')\n",
    "plt.bar(indices + 0.15, false_positives_pct, width=0.3, color='blue', label='False Positives')\n",
    "\n",
    "# Plot average lines\n",
    "plt.axhline(avg_false_negatives, color='darkred', linestyle='--', \n",
    "            label=f'Avg False Negatives: {avg_false_negatives:.2f}')\n",
    "plt.axhline(avg_false_positives, color='darkblue', linestyle='--', \n",
    "            label=f'Avg False Positives: {avg_false_positives:.2f}')\n",
    "\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Percentage of False Negatives and False Positives per Sample')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New cell: Visualization of samples with high error percentages\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define a threshold for high error (e.g., 30% error)\n",
    "threshold = 0.8\n",
    "\n",
    "# Filter samples with either false negative or false positive percentage above the threshold\n",
    "high_error_results = [\n",
    "    res for res in results_zero_shot \n",
    "    if res.get('percentage_false_negatives', 0) > threshold or res.get('percentage_false_positives', 0) > threshold\n",
    "]\n",
    "\n",
    "print(f\"Number of high error samples: {len(high_error_results)}\")\n",
    "\n",
    "for idx, res in enumerate(high_error_results):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # --- Left: Ground truth mask with false negative outlines --- \n",
    "    mask = res['mask']\n",
    "    if mask.ndim == 2:\n",
    "        gt_img = (mask * 255).astype(np.uint8)\n",
    "        gt_img = cv2.cvtColor(gt_img, cv2.COLOR_GRAY2RGB)\n",
    "    else:\n",
    "        gt_img = mask.copy()\n",
    "    ax1.imshow(gt_img)\n",
    "    ax1.set_title(\"Ground Truth with False Negatives\")\n",
    "    \n",
    "    for bbox in res.get('false_negatives', []):\n",
    "        rect = patches.Rectangle((bbox[0], bbox[1]),\n",
    "                                 bbox[2] - bbox[0],\n",
    "                                 bbox[3] - bbox[1],\n",
    "                                 linewidth=2,\n",
    "                                 edgecolor='r',\n",
    "                                 facecolor='none')\n",
    "        ax1.add_patch(rect)\n",
    "        \n",
    "    # --- Right: Predicted mask with false positive outlines ---\n",
    "    pred_mask = res['calculated_mask']\n",
    "    if pred_mask.ndim == 2:\n",
    "        pred_img = (pred_mask * 255).astype(np.uint8)\n",
    "        pred_img = cv2.cvtColor(pred_img, cv2.COLOR_GRAY2RGB)\n",
    "    else:\n",
    "        pred_img = pred_mask.copy()\n",
    "    ax2.imshow(pred_img)\n",
    "    ax2.set_title(\"Prediction with False Positives\")\n",
    "    \n",
    "    for bbox in res.get('false_positives', []):\n",
    "        rect = patches.Rectangle((bbox[0], bbox[1]),\n",
    "                                 bbox[2] - bbox[0],\n",
    "                                 bbox[3] - bbox[1],\n",
    "                                 linewidth=2,\n",
    "                                 edgecolor='b',\n",
    "                                 facecolor='none')\n",
    "        ax2.add_patch(rect)\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "    if idx >= 10:  # Limit to first 5 high error samples for visualization\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
