{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_from_disk\n",
    "import pytorch_lightning as pl\n",
    "import sys\n",
    "import os\n",
    "# Add the directory containing lit_sam_model.py to the Python path\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "from model.adapterModel import LitSamModel\n",
    "from model.samDataset import SAMDataset, SAMDataset3\n",
    "from utils.statistics import calculate_correlation\n",
    "from helperFunctions import *\n",
    "from model.inputTypes import InputTypes\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Get the path of the script\n",
    "current_file = Path(__file__).resolve() # src/training/your_script.py\n",
    "\n",
    "# 2. Go up one level to 'src', then into 'config'\n",
    "config_path = current_file.parent.parent / \"config\" / \"config_general.yaml\"\n",
    "\n",
    "# 3. Load the YAML\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# 4. Resolve the root of the project (one level above 'src')\n",
    "# This ensures that \"./data\" in the YAML is interpreted relative to the Project_Root\n",
    "PROJECT_ROOT = current_file.parent.parent.parent\n",
    "os.chdir(PROJECT_ROOT) \n",
    "\n",
    "# Extract paths from YAML\n",
    "DATA_DIR = config['paths']['data']\n",
    "CHECKPOINT_DIR = config['paths']['checkpoints']\n",
    "SAM_CHECKPOINT = config['paths']['sam_checkpoint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the test dataset\n",
    "\n",
    "test_dataset = load_from_disk(os.path.join(DATA_DIR, \"datasetTestFinal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamModel, SamConfig, SamProcessor\n",
    "import torch\n",
    "\n",
    "sam_checkpoint = os.path.join(CHECKPOINT_DIR, \"/firstencoder/sam-adapter-VV-epoch=89-val_loss=0.222-val_iou=0.599.ckpt\")\n",
    "\n",
    "# Create an instance of the model architecture with the loaded configuration\n",
    "model1 = LitSamModel.load_from_checkpoint(sam_checkpoint, model_name=\"facebook/sam-vit-base\", normalize = True, adapt = True, input_type=InputTypes.VV, HQ = False)\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.minor_models.selfSupervisedTwoEncodersModel import selfSupSamModel\n",
    "\n",
    "#Initialize the  self sup model\n",
    "#/home/gelato/Avalanche-Segmentation-with-Sam/code/training/checkpoints/mmsamsecondencoder/sam-selfsup-1-MMSAM-epoch=63-val_loss=0.001-val_iou=0.000.ckpt\n",
    "#\n",
    "#\n",
    "sam_checkpoint = sam_checkpoint = os.path.join(CHECKPOINT_DIR, \"/mmsamsecondencoder/sam-selfsup-3-MMSAM-epoch=62-val_loss=0.001-val_iou=0.000.ckpt\")\n",
    "\n",
    "image_encoder = LitSamModel(model_name=\"vit_b\", normalize=True, learning_rate=1e-5, adapt_patch_embed=False, input_type=InputTypes.VH, adapt=True).model.image_encoder\n",
    "target_image_encoder = LitSamModel(model_name=\"vit_b\", normalize=True, learning_rate=1e-5, adapt_patch_embed=False, input_type=InputTypes.VH, adapt=True).model.image_encoder\n",
    "\n",
    "# Create an instance of the model architecture with the loaded configuration\n",
    "selfsupmodel = selfSupSamModel.load_from_checkpoint(sam_checkpoint, model_name=\"vit-b\", normalize = True, adapt_patch_embed=False, image_encoder = image_encoder, target_image_encoder = target_image_encoder)\n",
    "\n",
    "# Extract only the image encoder\n",
    "encoder = selfsupmodel.model.image_encoder\n",
    "decoder = model1.model.mask_decoder\n",
    "\n",
    "# Create an instance of the model architecture with the loaded configuration\n",
    "model2 = LitSamModel(model_name=\"facebook/sam-vit-base\", normalize = True, input_type=InputTypes.VH, adapt = True, encoder = encoder, decoder = decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard SAM with VH training on freezed decoder\n",
    "\n",
    "sam_checkpoint = os.path.join(CHECKPOINT_DIR, \"/ourssecondencoder/sam-adapter-complementaryVH-1-epoch=50-val_loss=0.245-val_iou=0.575.ckpt\")\n",
    "\n",
    "# Create an instance of the model architecture with the loaded configuration\n",
    "model2 = LitSamModel.load_from_checkpoint(sam_checkpoint, model_name=\"facebook/sam-vit-base\", normalize = True, input_type=InputTypes.VH, adapt = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(mask1, mask2):\n",
    "\n",
    "    # Ensure the masks are PyTorch tensors\n",
    "    if isinstance(mask1, np.ndarray):\n",
    "        mask1 = torch.tensor(mask1)\n",
    "    if isinstance(mask2, np.ndarray):\n",
    "        mask2 = torch.tensor(mask2)\n",
    "        \n",
    "    # Ensure the masks are binary\n",
    "    mask1 = mask1 > 0\n",
    "    mask2 = mask2 > 0\n",
    "    \n",
    "    # Calculate the intersection and union\n",
    "    intersection = torch.logical_and(mask1, mask2)\n",
    "    union = torch.logical_or(mask1, mask2)\n",
    "    \n",
    "    # Compute the IoU\n",
    "    iou = torch.sum(intersection).float() / torch.sum(union).float()\n",
    "    \n",
    "    return iou, intersection, union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.sumModel import SumSamModel, SAMDataset as SumDataset, SAMDataset3 as SumDataset3\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "sumModel = SumSamModel(models = [model1, model2], model_name=\"vit-b\", normalize = True, HQ = True)\n",
    "\n",
    "# Create an instance of the SAMDataset\n",
    "test_dataset_sam = SumDataset3(dataset=test_dataset, processor=processor, augment=False, test = True)\n",
    "\n",
    "# Create a DataLoader instance for the validation dataset\n",
    "test_sum_dataloader = DataLoader(test_dataset_sam, batch_size=5, shuffle=False, num_workers=8, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.sumModel import SumSamModel, SAMDataset as SumDataset, SAMDataset3 as SumDataset3\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model1.to(device)\n",
    "model2.to(device)\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#/home/gelato/Avalanche-Segmentation-with-Sam/code/training/checkpoints/sfg/sam-sfg-2-epoch=48-val_loss=0.204-val_iou=0.626.ckpt\n",
    "#\n",
    "sam_checkpoint = os.path.join(CHECKPOINT_DIR, \"/sfg/sam-sfg-1-epoch=06-val_loss=0.201-val_iou=0.624.ckpt\")\n",
    "sumModel = SumSamModel.load_from_checkpoint(sam_checkpoint, models = [model1, model2], model_name=\"vit-b\", normalize = True, HQ = False)\n",
    "\n",
    "sumModel.to(device)\n",
    "\n",
    "# Create an instance of the SAMDataset\n",
    "test_dataset_sam = SumDataset3(dataset=test_dataset, processor=processor, augment=False, test = True)\n",
    "\n",
    "# Create a DataLoader instance for the validation dataset\n",
    "test_sum_dataloader = DataLoader(test_dataset_sam, batch_size=5, shuffle=False, num_workers=8, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a trainer\n",
    "trainer3 = pl.Trainer(accelerator='gpu', devices=1)\n",
    "\n",
    "# Run the evaluation\n",
    "trainer3.test(sumModel, dataloaders=test_sum_dataloader, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the stored results\n",
    "test_results3 = sumModel.test_results\n",
    "ground_truth_masks3 = test_results3['ground_truth_masks']\n",
    "predicted_masks3 = test_results3['predicted_masks']\n",
    "individual_ious3 = test_results3['individual_ious']\n",
    "bboxes3 = test_results3['bboxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_masks3 = np.concatenate(predicted_masks3, axis=0)\n",
    "ground_truth_masks3 = np.concatenate(ground_truth_masks3, axis=0)\n",
    "bounding_boxes3 = np.concatenate(bboxes3, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_indices = list(range(384))  # choose first 384 examples for instance\n",
    "test_dataset_subset = test_dataset.select(subset_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_zero_shot3 = generate_results(test_dataset_subset, ground_truth_masks3, predicted_masks3, bounding_boxes3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove exemples with bounding boxes (0,0,512,512)\n",
    "results_zero_shot_copy3 = results_zero_shot3.copy()\n",
    "results_zero_shot3 = [res for res in results_zero_shot3 if not (res['bbox'] == [0, 0, 1024, 1024]).all()]\n",
    "results_zero_shot_general3 = [res for res in results_zero_shot_copy3 if (res['bbox'] == [0, 0, 1024, 1024]).all()]\n",
    "\n",
    "results, metrics =calculate_pixel_based_metrics(results_zero_shot3)\n",
    "\n",
    "print(metrics)\n",
    "\n",
    "results, metrics =calculate_pixel_based_metrics(results_zero_shot_general3)\n",
    "\n",
    "\n",
    "print(metrics)\n",
    "\n",
    "results_zero_shot = results_zero_shot_copy3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the list of dictionaries\n",
    "df_finetune3 = pd.DataFrame(results_zero_shot3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows with None IoU values\n",
    "df_filtered3 = df_finetune3.dropna(subset=['iou'])\n",
    "\n",
    "\n",
    "# Create a boolean mask for rows with bbox (0,0,512,512) using apply\n",
    "mask_bbox3 = df_filtered3['bbox'].apply(\n",
    "    lambda b: np.all(np.array(b) == np.array((0, 0, 1024, 1024)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered3 = df_filtered3[~mask_bbox3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered3 = df_filtered3[mask_bbox3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iou_statistics(df_filtered3, model_name='SUM model sfg HQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iou_statistics(df_filtered3, model_name='SUM model sfg MM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iou_statistics(df_filtered3, model_name='SUM model sfg full segmentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iou_statistics(df_filtered3, model_name='SUM model sfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iou_statistics(df_filtered3, model_name='SUM model MM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iou_statistics(df_filtered3, model_name='SUM model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mask_area_vs_iou(df_filtered3, model_name='SUM model sfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iou_vs_num_avalanches(df_filtered3, model_name='SUM model sfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered3['bbox'] = df_filtered3['bbox'].apply(lambda b: np.squeeze(b) if np.array(b).ndim == 2 else b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iou_vs_area_ratio(df_filtered3, model_name='SUM model sfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_mask_area_iou_correlation(df_filtered3, calculate_correlation, model_name='SUM model sfg ')\n",
    "compute_mask_area_iou_correlation(df_filtered3, calculate_correlation, model_name='SUM model sfg', scale='log')\n",
    "compute_num_avalanche_iou_correlation(df_filtered3, calculate_correlation, model_name='SUM model sfg ')\n",
    "compute_area_ratio_iou_correlation(df_filtered3, calculate_correlation, model_name='SUM model sfg ')\n",
    "compute_area_ratio_iou_correlation(df_filtered3, calculate_correlation, model_name='SUM model sfg', scale='log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find bounding boxes for each group of disconnected white pixels\n",
    "def find_bounding_boxes(mask):\n",
    "    # Ensure the mask is an 8-bit image.\n",
    "    if mask.dtype != \"uint8\":\n",
    "        # If mask values are in range 0-1, scale them by 255\n",
    "        if mask.max() <= 1:\n",
    "            mask_uint8 = (mask * 255).astype('uint8')\n",
    "        else:\n",
    "            mask_uint8 = mask.astype('uint8')\n",
    "    else:\n",
    "        mask_uint8 = mask\n",
    "\n",
    "    # Find contours in the binary mask\n",
    "    contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Compute bounding boxes for each contour and convert (x, y, w, h) to (x_min, y_min, x_max, y_max)\n",
    "    bounding_boxes = []\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        bounding_boxes.append([x, y, x + w, y + h])\n",
    "    return bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treshold = 0.3\n",
    "\n",
    "for res in results_zero_shot3:\n",
    "    false_positives = []\n",
    "    false_negatives = []\n",
    "    # The predicted and ground truth avalanche masks\n",
    "    pred_mask = res['calculated_mask']\n",
    "    true_mask = res['mask']\n",
    "\n",
    "    original_bbox = find_bounding_boxes(true_mask)\n",
    "    pred_bbox = find_bounding_boxes(pred_mask)\n",
    "\n",
    "    for bbox in original_bbox:\n",
    "        predicted = False\n",
    "        for pred in pred_bbox:\n",
    "\n",
    "            pred_mask_local = np.zeros_like(pred_mask)\n",
    "            pred_mask_local[pred[1]:pred[3], pred[0]:pred[2]] = pred_mask[pred[1]:pred[3], pred[0]:pred[2]]\n",
    "\n",
    "            original_mask_local = np.zeros_like(true_mask)\n",
    "            original_mask_local[bbox[1]:bbox[3], bbox[0]:bbox[2]] = true_mask[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
    "\n",
    "            # Calculate the overlap area\n",
    "            overlap_area = np.sum(np.logical_and(pred_mask_local, original_mask_local))\n",
    "            original_area = np.sum(original_mask_local)\n",
    "\n",
    "            overlap = overlap_area / original_area if original_area > 0 else 0\n",
    "            if overlap > treshold:\n",
    "                predicted = True\n",
    "                break\n",
    "        if not predicted:\n",
    "            false_negatives.append(bbox)\n",
    "    for bbox in pred_bbox:\n",
    "        original = False\n",
    "        for originalbbox in original_bbox:\n",
    "            \n",
    "            pred_mask_local = np.zeros_like(pred_mask)\n",
    "            pred_mask_local[bbox[1]:bbox[3], bbox[0]:bbox[2]] = pred_mask[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
    "\n",
    "            original_mask_local = np.zeros_like(true_mask)\n",
    "            original_mask_local[originalbbox[1]:originalbbox[3], originalbbox[0]:originalbbox[2]] = true_mask[originalbbox[1]:originalbbox[3], originalbbox[0]:originalbbox[2]]\n",
    "\n",
    "            # Calculate the overlap area\n",
    "            overlap_area = np.sum(np.logical_and(pred_mask_local, original_mask_local))\n",
    "            pred_area = np.sum(pred_mask_local)\n",
    "\n",
    "            overlap = overlap_area / pred_area if pred_area > 0 else 0\n",
    "            if overlap > treshold:\n",
    "                original = True\n",
    "                break\n",
    "        if not original:\n",
    "            false_positives.append(bbox)\n",
    "            \n",
    "    # Add the results into the current dictionary element\n",
    "    res['false_negatives'] = false_negatives\n",
    "    res['false_positives'] = false_positives\n",
    "    res['percentage_false_negatives'] = len(false_negatives) / len(original_bbox) if len(original_bbox) > 0 else 0\n",
    "    res['percentage_false_positives'] = len(false_positives) / len(pred_bbox) if len(pred_bbox) > 0 else 0\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helperFunctions import *\n",
    "\n",
    "results_zero_shot3 = compute_error_percentages_iou(results_zero_shot3, iou_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove exemples with bounding boxes (0,0,512,512)\n",
    "results_zero_shot_copy = results_zero_shot3.copy()\n",
    "results_zero_shot3 = [res for res in results_zero_shot3 if not (res['bbox'] == [0, 0, 1024, 1024]).all()]\n",
    "results_zero_shot_general = [res for res in results_zero_shot_copy if (res['bbox'] == [0, 0, 1024, 1024]).all()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_zero_shot3 = results_zero_shot_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New cell: Plot the percentage of false positives and false negatives with averages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Extract percentages from the results\n",
    "false_negatives_pct = [res.get('percentage_false_negatives', 0) for res in results_zero_shot3]\n",
    "false_positives_pct = [res.get('percentage_false_positives', 0) for res in results_zero_shot3]\n",
    "indices = np.arange(len(results_zero_shot3))\n",
    "\n",
    "# Compute average percentages\n",
    "avg_false_negatives = np.mean(false_negatives_pct)\n",
    "avg_false_positives = np.mean(false_positives_pct)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(indices - 0.15, false_negatives_pct, width=0.3, color='red', label='False Negatives')\n",
    "plt.bar(indices + 0.15, false_positives_pct, width=0.3, color='blue', label='False Positives')\n",
    "\n",
    "# Plot average lines\n",
    "plt.axhline(avg_false_negatives, color='darkred', linestyle='--', \n",
    "            label=f'Avg False Negatives: {avg_false_negatives:.2f}')\n",
    "plt.axhline(avg_false_positives, color='darkblue', linestyle='--', \n",
    "            label=f'Avg False Positives: {avg_false_positives:.2f}')\n",
    "\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title(f'Percentage of False Negatives and False Positives per Sample (Baseline, Treshold = {treshold})')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New cell: Plot the percentage of false positives and false negatives with averages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract percentages from the results\n",
    "false_negatives_pct = [res.get('percentage_false_negatives', 0) for res in results_zero_shot_general]\n",
    "false_positives_pct = [res.get('percentage_false_positives', 0) for res in results_zero_shot_general]\n",
    "indices = np.arange(len(results_zero_shot_general))\n",
    "\n",
    "# Compute average percentages\n",
    "avg_false_negatives = np.mean(false_negatives_pct)\n",
    "avg_false_positives = np.mean(false_positives_pct)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(indices - 0.15, false_negatives_pct, width=0.3, color='red', label='False Negatives')\n",
    "plt.bar(indices + 0.15, false_positives_pct, width=0.3, color='blue', label='False Positives')\n",
    "\n",
    "# Plot average lines\n",
    "plt.axhline(avg_false_negatives, color='darkred', linestyle='--', \n",
    "            label=f'Avg False Negatives: {avg_false_negatives:.2f}')\n",
    "plt.axhline(avg_false_positives, color='darkblue', linestyle='--', \n",
    "            label=f'Avg False Positives: {avg_false_positives:.2f}')\n",
    "\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Percentage of False Negatives and False Positives per Sample')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_thresholds = 50\n",
    "thresholds = np.linspace(0, 1, num_thresholds)\n",
    "optimal_thresholds = []\n",
    "gt_areas = []\n",
    "optimal_ious = []\n",
    "\n",
    "# You can also store IoU for each threshold if needed:\n",
    "results_threshold = []\n",
    "\n",
    "for index in range(384):\n",
    "\n",
    "    # Skip samples with a bounding box of [0, 0, 512, 512]\n",
    "    if np.array_equal(np.array(bounding_boxes3[index]), np.array([0, 0, 1024, 1024])):\n",
    "        continue\n",
    "    # Get the ground truth mask and predicted soft mask\n",
    "    mask = ground_truth_masks3[index]\n",
    "    sam_seg = predicted_masks3[index]\n",
    "    \n",
    "    # Convert predicted soft mask to probability map\n",
    "    sam_seg_prob = torch.sigmoid(torch.tensor(sam_seg))\n",
    "    sam_seg_prob_np = sam_seg_prob.cpu().numpy().squeeze()\n",
    "    \n",
    "    best_iou = -1\n",
    "    best_thr = None\n",
    "    \n",
    "    for thr in thresholds:\n",
    "        # Convert soft mask to binary using current threshold\n",
    "        pred_mask = (sam_seg_prob_np > thr).astype(np.uint8)\n",
    "        \n",
    "        # Calculate IoU; calculate_iou returns (iou, intersection, union)\n",
    "        iou, inter, union = calculate_iou(mask, pred_mask)\n",
    "        iou_val = iou.cpu().item() if isinstance(iou, torch.Tensor) else iou\n",
    "        \n",
    "        results_threshold.append({\n",
    "            'sample': index,\n",
    "            'threshold': thr,\n",
    "            'iou': iou_val\n",
    "        })\n",
    "        \n",
    "        if iou_val > best_iou:\n",
    "            best_iou = iou_val\n",
    "            best_thr = thr\n",
    "            \n",
    "    optimal_thresholds.append(best_thr)\n",
    "    optimal_ious.append(best_iou)  # Save the best IoU.\n",
    "    # Compute ground truth mask area (number of non-zero pixels)\n",
    "    area = np.sum(mask > 0)\n",
    "    gt_areas.append(area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# optimal_ious is a list containing the best IoU for each sample\n",
    "average_optimal_iou = sum(optimal_ious) / len(optimal_ious)\n",
    "\n",
    "# Bar chart for optimal IoU values per sample\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(optimal_ious)), optimal_ious, color='green')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Optimal IoU')\n",
    "plt.title('Optimal IoU Values for Different Samples')\n",
    "plt.text(0.5, 0.95, f'Average Optimal IoU: {average_optimal_iou:.4f}', \n",
    "         ha='center', va='center', transform=plt.gca().transAxes, \n",
    "         fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.show()\n",
    "\n",
    "# Line plot for optimal IoU values per sample\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(optimal_ious)), optimal_ious, marker='o', linestyle='-', color='green')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Optimal IoU')\n",
    "plt.title('Optimal IoU Values for Different Samples')\n",
    "plt.text(0.5, 0.95, f'Average Optimal IoU: {average_optimal_iou:.4f}', \n",
    "         ha='center', va='center', transform=plt.gca().transAxes, \n",
    "         fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
