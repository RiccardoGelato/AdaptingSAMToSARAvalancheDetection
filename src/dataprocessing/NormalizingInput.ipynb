{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Get the path of the script\n",
    "current_file = Path(__file__).resolve() # src/training/your_script.py\n",
    "\n",
    "# 2. Go up one level to 'src', then into 'config'\n",
    "config_path = current_file.parent.parent / \"config\" / \"config_general.yaml\"\n",
    "\n",
    "# 3. Load the YAML\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# 4. Resolve the root of the project (one level above 'src')\n",
    "# This ensures that \"./data\" in the YAML is interpreted relative to the Project_Root\n",
    "PROJECT_ROOT = current_file.parent.parent.parent\n",
    "os.chdir(PROJECT_ROOT) \n",
    "\n",
    "# Extract paths from YAML\n",
    "DATA_DIR = config['paths']['data']\n",
    "CHECKPOINT_DIR = config['paths']['checkpoints']\n",
    "SAM_CHECKPOINT = config['paths']['sam_checkpoint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_from_disk\n",
    "from transformers import SamModel, SamProcessor, SamConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAMDataset(Dataset):\n",
    "  \"\"\"\n",
    "  This class is used to create a dataset that serves input images and masks.\n",
    "  It takes a dataset and a processor as input and overrides the __len__ and __getitem__ methods of the Dataset class.\n",
    "  \"\"\"\n",
    "  def __init__(self, dataset, processor):\n",
    "    self.dataset = dataset\n",
    "    self.processor = processor\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = self.dataset[idx]\n",
    "    VH0 = np.array(item[\"VH0\"])\n",
    "    VH1 = np.array(item[\"VH1\"])\n",
    "    VV0 = np.array(item[\"VV0\"])\n",
    "    VV1 = np.array(item[\"VV1\"])\n",
    "    dem = np.array(item[\"dem\"])\n",
    "    slope = np.array(item[\"slope\"])\n",
    "    ground_truth_mask = np.array(item[\"label\"], np.float32)\n",
    "\n",
    "    # get bounding box prompt\n",
    "    prompt = item[\"box\"]\n",
    "    xchange = np.random.randint(-5, 5)\n",
    "    ychange = np.random.randint(-5, 5)\n",
    "    wchange = np.random.randint(-5, 5)\n",
    "    hchange = np.random.randint(-5, 5)\n",
    "    input_boxes = []\n",
    "    for box in prompt: \n",
    "      x, y, w, h = box\n",
    "      input_boxes.append([[max(0, x + xchange), max(0, y + ychange), min(512, x + w + wchange), min(512, y + h + hchange)]])\n",
    "\n",
    "    # Combine all channels to create image\n",
    "    image = np.stack([VH0, VH1, VV0, VV1, dem, slope], axis=1)\n",
    "    image_tensor = torch.from_numpy(image).float()  # Convert to tensor and change to (C, H, W)\n",
    "    ground_truth_mask = torch.from_numpy(ground_truth_mask)\n",
    "    \n",
    "    inputs = {\n",
    "            \"pixel_values\": image_tensor,\n",
    "            \"ground_truth_mask\": ground_truth_mask,\n",
    "        }\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def calculate_mean_std(dataloader):\n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    total_images_count = 0\n",
    "    \n",
    "\n",
    "    for batch in dataloader:\n",
    "        images = batch[\"pixel_values\"]\n",
    "        # Assuming images is a batch of images with shape (batch_size, channels, height, width)\n",
    "        batch_samples = images.size(0)  # batch size (the last batch can have smaller size)\n",
    "        images = images.view(batch_samples, images.size(1), -1)  # reshape to (batch_size, channels, height*width)\n",
    "        mean += images.mean(2).sum(0)  # sum over batch and height*width\n",
    "        std += images.std(2).sum(0)  # sum over batch and height*width\n",
    "        total_images_count += batch_samples\n",
    "\n",
    "    mean /= total_images_count\n",
    "    std /= total_images_count\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "with open(os.path.join(DATA_DIR, \"processed\", \"train_indices.pkl\"), \"rb\") as f:\n",
    "    train_indices = pickle.load(f)\n",
    "\n",
    "train_dataset = load_from_disk(os.path.join(DATA_DIR, \"processed\", \"train_dataset\"))    \n",
    "\n",
    "# Used to train only on samples with the full mask\n",
    "all_indices = set(range(len(train_dataset)))\n",
    "exclude_indices = set(train_indices)\n",
    "train_indices = list(all_indices - exclude_indices)\n",
    "\n",
    "# Filter the dataset to remove samples with a bbox of [0,0,512,512]\n",
    "train_dataset = train_dataset.select(train_indices)\n",
    "\n",
    "# Initialize the processor\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "# Create an instance of the SAMDataset\n",
    "train_dataset_sam = SAMDataset(dataset=train_dataset, processor=processor)\n",
    "\n",
    "# Create a DataLoader instance for the training dataset\n",
    "train_dataloader = DataLoader(train_dataset_sam, batch_size=4, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and std for the training dataset\n",
    "mean, std = calculate_mean_std(train_dataloader)\n",
    "\n",
    "# Convert mean and std to lists\n",
    "mean = mean.tolist()\n",
    "std = std.tolist()\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Std: {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "# Get the first image, mask, and boxes from the batch\n",
    "image = batch[\"pixel_values\"][0]\n",
    "\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_test = [0.5111843347549438, 0.5049981474876404, 0.1675393283367157]\n",
    "std_test = [0.25912609696388245, 0.26219138503074646, 0.06871911138296127]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_normalized = transforms.Normalize(mean=mean_test, std=std_test)(image)\n",
    "print(image_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_mean = torch.Tensor(mean_test).view(-1, 1, 1)\n",
    "tensor_std = torch.Tensor(std_test).view(-1, 1, 1)\n",
    "print(tensor_mean)\n",
    "print(tensor_std)\n",
    "x = (batch[\"pixel_values\"] - tensor_mean) / tensor_std \n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Equal: \", torch.equal(image_normalized, x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_normalized = transforms.Normalize(mean=mean_test, std=std_test)(batch[\"pixel_values\"])\n",
    "print(batch_normalized[0])\n",
    "print(\"Equal: \", torch.equal(x[0], batch_normalized[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
