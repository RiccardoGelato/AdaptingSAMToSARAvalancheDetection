{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Code to import necessary libraries such as json, glob, pandas, matplotlib.pyplot, and seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import seaborn as sns  # For enhanced data visualization\n",
    "\n",
    "# Configure seaborn for better aesthetics\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Add the path to the custom module for data processing \n",
    "import dataprocessing.creationOfDataframe as dp  # Custom module for data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Get the path of the script\n",
    "current_file = Path(__file__).resolve() # src/training/your_script.py\n",
    "\n",
    "# 2. Go up one level to 'src', then into 'config'\n",
    "config_path = current_file.parent.parent / \"config\" / \"config_general.yaml\"\n",
    "\n",
    "# 3. Load the YAML\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# 4. Resolve the root of the project (one level above 'src')\n",
    "# This ensures that \"./data\" in the YAML is interpreted relative to the Project_Root\n",
    "PROJECT_ROOT = current_file.parent.parent.parent\n",
    "os.chdir(PROJECT_ROOT) \n",
    "\n",
    "# Extract paths from YAML\n",
    "DATA_DIR = config['paths']['data']\n",
    "CHECKPOINT_DIR = config['paths']['checkpoints']\n",
    "SAM_CHECKPOINT = config['paths']['sam_checkpoint']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Merge JSON Files\n",
    "Code to use glob to locate files with pattern '*_met.json', read each file using json.load(), and merge the data into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "root_dir = DATA_DIR + '/avalanche_input'\n",
    "\n",
    "json_paths = dp.get_all_image_paths(root_dir, ['met.json'])\n",
    "\n",
    "print(\"Number of JSON files:\", len(json_paths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, loop over these file paths to read and flatten your JSON data.\n",
    "records = []\n",
    "count_mismatch = 0\n",
    "for file in json_paths:\n",
    "    # Ensure the file exists\n",
    "    if not os.path.exists(file):\n",
    "        print(f\"File does not exist: {file}\")\n",
    "        continue\n",
    "    with open(file, \"r\") as f:\n",
    "        met_data = json.load(f)\n",
    "    # Assuming JSON keys: \"time\", \"air_temperature_2m\", etc.\n",
    "    times = met_data.get(\"time\", [])\n",
    "    air_temp = met_data.get(\"air_temperature_2m\", [])\n",
    "    precip = met_data.get(\"precipitation_amount\", [])\n",
    "    wind_speed = met_data.get(\"wind_speed_10m\", [])\n",
    "    humidity = met_data.get(\"relative_humidity_2m\", [])\n",
    "    air_pressure = met_data.get(\"air_pressure_at_sea_level\", [])\n",
    "\n",
    "    #print(len(times), len(air_temp), len(precip), len(wind_speed), len(humidity), len(air_pressure))\n",
    "    # Check if all lists have the same length\n",
    "    if not (len(times) == len(air_temp) == len(precip) == len(wind_speed) == len(humidity) == len(air_pressure)):\n",
    "        print(f\"Data length mismatch in file: {file}\")\n",
    "        count_mismatch += 1\n",
    "        continue\n",
    "    record = {\n",
    "        \"time\": times,\n",
    "        \"air_temperature_2m\": air_temp,\n",
    "        \"precipitation_amount\": precip,\n",
    "        \"wind_speed_10m\": wind_speed,\n",
    "        \"relative_humidity_2m\": humidity,\n",
    "        \"air_pressure_at_sea_level\": air_pressure,\n",
    "        \"source_file\": file\n",
    "    }\n",
    "    records.append(record)\n",
    "    \n",
    "#print(\"Records\", records)\n",
    "# Create a DataFrame from all records.\n",
    "df_met = pd.DataFrame(records)\n",
    "print(\"Meteorological DataFrame shape:\", df_met.shape)\n",
    "df_met.head()\n",
    "print(\"Number of mismatched files:\", count_mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataframe is not empty\n",
    "if df_met.empty:\n",
    "    raise ValueError(\"The DataFrame is empty. Please check the JSON files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_met.to_pickle(DATA_DIR + '/df_met.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = ['air_temperature_2m', 'precipitation_amount', 'wind_speed_10m', 'relative_humidity_2m', 'air_pressure_at_sea_level']\n",
    "\n",
    "mean_values = df_met[numerical_columns].mean()\n",
    "print(\"Mean values of numerical columns:\")\n",
    "print(mean_values)\n",
    "std_values = df_met[numerical_columns].std()\n",
    "print(\"Standard deviation values of numerical columns:\")\n",
    "print(std_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = ['air_temperature_2m', 'precipitation_amount', 'wind_speed_10m', 'relative_humidity_2m', 'air_pressure_at_sea_level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def flatten_column(column_series):\n",
    "    # Assuming each cell contains a list, concatenate all lists into one array\n",
    "    return np.concatenate(column_series.values)\n",
    "\n",
    "print(\"Mean values of numerical columns:\")\n",
    "for col in numerical_columns:\n",
    "    flat_values = flatten_column(df_met[col])\n",
    "    print(f\"{col}: {flat_values.mean()}\")\n",
    "\n",
    "print(\"Standard deviation values of numerical columns:\")\n",
    "for col in numerical_columns:\n",
    "    flat_values = flatten_column(df_met[col])\n",
    "    print(f\"{col}: {flat_values.std()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
