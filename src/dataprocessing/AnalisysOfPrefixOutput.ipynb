{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437697dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from transformers import SamProcessor\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import sys\n",
    "# Add the directory containing lit_sam_model.py to the Python path\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "from model.minor_models.prefixModel import LitSamModel\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adc3669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Get the path of the script\n",
    "current_file = Path(__file__).resolve() # src/training/your_script.py\n",
    "\n",
    "# 2. Go up one level to 'src', then into 'config'\n",
    "config_path = current_file.parent.parent / \"config\" / \"config_general.yaml\"\n",
    "\n",
    "# 3. Load the YAML\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# 4. Resolve the root of the project (one level above 'src')\n",
    "# This ensures that \"./data\" in the YAML is interpreted relative to the Project_Root\n",
    "PROJECT_ROOT = current_file.parent.parent.parent\n",
    "os.chdir(PROJECT_ROOT) \n",
    "\n",
    "# Extract paths from YAML\n",
    "DATA_DIR = config['paths']['data']\n",
    "CHECKPOINT_DIR = config['paths']['checkpoints']\n",
    "SAM_CHECKPOINT = config['paths']['sam_checkpoint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23bdb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_from_disk(os.path.join(DATA_DIR, 'datasetTestFinal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8986432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamModel, SamConfig, SamProcessor\n",
    "import torch\n",
    "\n",
    "sam_checkpoint = os.path.join(CHECKPOINT_DIR, \"/prefix/sam-float-adapter-smallprefix-model-epoch=74-val_loss=0.212-val_iou=0.606.ckpt\")\n",
    "\n",
    "# Create an instance of the model architecture with the loaded configuration\n",
    "model = LitSamModel.load_from_checkpoint(sam_checkpoint, model_name=\"vit-b\", normalize = True, adapt = True, freeze = False, num_classes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dcb2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = model.model.prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cf8578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_raw_input(array):\n",
    "    # Display the images\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    plt.subplot(3, 2, 1)\n",
    "    plt.imshow(array[:, :, 0], cmap='gray')\n",
    "    plt.title(\"HV0\")\n",
    "\n",
    "    plt.subplot(3, 2, 2)\n",
    "    plt.imshow(array[:, :, 1], cmap='gray')\n",
    "    plt.title(\"HV1\")\n",
    "\n",
    "    plt.subplot(3, 2, 3)\n",
    "    plt.imshow(array[:, :, 2], cmap='gray')\n",
    "    plt.title(\"VV0\")\n",
    "\n",
    "    plt.subplot(3, 2, 4)\n",
    "    plt.imshow(array[:, :, 3], cmap='gray')\n",
    "    plt.title(\"VV1\")\n",
    "\n",
    "    plt.subplot(3, 2, 5)\n",
    "    plt.imshow(array[:, :, 4], cmap='gray')\n",
    "    plt.title(\"DEM\")\n",
    "\n",
    "    plt.subplot(3, 2, 6)\n",
    "    plt.imshow(array[:, :, 5], cmap='gray')\n",
    "    plt.title(\"SLOPE\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5a5075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prefix_output(prefix, sample):\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Convert sample \"image\" field into a NumPy array (if not already)\n",
    "    ground_truth_mask = np.array(sample[\"label\"])\n",
    "    VH0 = np.array(sample[\"VH0\"])\n",
    "    VH1 = np.array(sample[\"VH1\"])\n",
    "    VV0 = np.array(sample[\"VV0\"])\n",
    "    VV1 = np.array(sample[\"VV1\"])\n",
    "    dem = np.array(sample[\"dem\"])\n",
    "    slope = np.array(sample[\"slope\"])\n",
    "\n",
    "    image = np.stack([VH0, VH1, VV0, VV1, dem, slope], axis=-1)\n",
    "\n",
    "    # Create a batch dimension (assumes image shape is (H, W, C))\n",
    "    image_batch = np.expand_dims(image, axis=0)\n",
    "    \n",
    "    # Convert to tensor and permute to shape (batch, C, H, W)\n",
    "    image_tensor = torch.from_numpy(image_batch).permute(0, 3, 1, 2).float()\n",
    "\n",
    "    # Move tensor to same device as prefix module\n",
    "    device = next(prefix.parameters()).device\n",
    "    image_tensor = image_tensor.to(device)\n",
    "\n",
    "    # Pass the image through the prefix module (inference mode)\n",
    "    with torch.no_grad():\n",
    "        prefix_output = prefix(image_tensor)\n",
    "\n",
    "    # Process the output: assume shape [batch, 3, H, W], take first output\n",
    "    output_image = prefix_output[0].cpu()\n",
    "\n",
    "    # Normalize output to [0, 1]\n",
    "    #output_image = (output_image - output_image.min()) / (output_image.max() - output_image.min())\n",
    "\n",
    "    # Change shape from (C, H, W) to (H, W, C)\n",
    "    output_image_np = output_image.permute(1, 2, 0).numpy()\n",
    "\n",
    "    # Visualize the RGB image\n",
    "    plt.imshow(output_image_np)\n",
    "    plt.title(\"Prefix Module Output as RGB\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1a4479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prefix_channels(prefix, sample):\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Convert sample \"image\" field into a NumPy array (if not already)\n",
    "    VH0 = np.array(sample[\"VH0\"])\n",
    "    VH1 = np.array(sample[\"VH1\"])\n",
    "    VV0 = np.array(sample[\"VV0\"])\n",
    "    VV1 = np.array(sample[\"VV1\"])\n",
    "    dem = np.array(sample[\"dem\"])\n",
    "    slope = np.array(sample[\"slope\"])\n",
    "    \n",
    "    image = np.stack([VH0, VH1, VV0, VV1, dem, slope], axis=-1)\n",
    "\n",
    "    # Create a batch dimension and convert to tensor with shape (B, C, H, W)\n",
    "    image_tensor = torch.from_numpy(np.expand_dims(image, axis=0)).permute(0, 3, 1, 2).float()\n",
    "\n",
    "    # Send the tensor to the same device as your prefix module\n",
    "    device = next(prefix.parameters()).device\n",
    "    image_tensor = image_tensor.to(device)\n",
    "\n",
    "    # Forward pass through the prefix module\n",
    "    with torch.no_grad():\n",
    "        prefix_output = prefix(image_tensor)\n",
    "    \n",
    "    # Assume prefix_output shape is [B, C, H, W] (for example, 3 channels)\n",
    "    output_tensor = prefix_output[0].cpu()  # shape: (C, H, W)\n",
    "\n",
    "    # Independently plot each channel\n",
    "    num_channels = output_tensor.shape[0]\n",
    "    fig, axes = plt.subplots(1, num_channels, figsize=(4 * num_channels, 4))\n",
    "    if num_channels == 1:\n",
    "        axes = [axes]\n",
    "    for i in range(num_channels):\n",
    "        channel_img = output_tensor[i].numpy()\n",
    "        axes[i].imshow(channel_img, cmap='viridis')\n",
    "        axes[i].set_title(f\"Channel {i}\")\n",
    "        axes[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eefc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_normal_image(array):\n",
    "    # Display the images\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(array)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39325fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mask(sample):\n",
    "    # Assuming sample has a \"mask\" field that is a binary mask\n",
    "    mask = np.array(sample['label'])\n",
    "\n",
    "    # Display the mask\n",
    "    plt.imshow(mask, cmap='gray')\n",
    "    plt.title(\"Mask\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f1cd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = test_dataset[20]\n",
    "ground_truth_mask = np.array(item[\"label\"])\n",
    "VH0 = np.array(item[\"VH0\"])\n",
    "VH1 = np.array(item[\"VH1\"])\n",
    "VV0 = np.array(item[\"VV0\"])\n",
    "VV1 = np.array(item[\"VV1\"])\n",
    "dem = np.array(item[\"dem\"])\n",
    "slope = np.array(item[\"slope\"])\n",
    "\n",
    "image = np.stack([VH0, VH1, VV0, VV1, dem, slope], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0bc98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_prefix_channels(prefix, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba29f954",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_raw_input(image)\n",
    "visualize_prefix_output(prefix, item)\n",
    "visualize_mask(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b384a988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_n_complete_samples(dataset1, n=23):\n",
    "    counter = 0\n",
    "    for i in range(dataset1.__len__()):\n",
    "        if counter >= n:\n",
    "            break\n",
    "        sample = dataset1[i]\n",
    "        if(sample[\"box\"] == [0, 0, 512, 512]):\n",
    "            counter += 1\n",
    "        else:\n",
    "            continue\n",
    "        if counter < 22:\n",
    "            continue\n",
    "        VH0 = np.array(sample[\"VH0\"])\n",
    "        VH1 = np.array(sample[\"VH1\"])\n",
    "        VV0 = np.array(sample[\"VV0\"])\n",
    "        VV1 = np.array(sample[\"VV1\"])\n",
    "        dem = np.array(sample[\"dem\"])\n",
    "        slope = np.array(sample[\"slope\"])\n",
    "        mask = np.array(sample['label'])\n",
    "\n",
    "        image = np.stack([VH0, VH1, VV0, VV1, dem, slope], axis=-1)\n",
    "        #show_raw_input(image)\n",
    "        visualize_prefix_output(prefix, sample)\n",
    "        \n",
    "        # Display DEM\n",
    "        plt.figure(figsize=(10, 10))  # Increase figure size for higher quality\n",
    "        plt.imshow(dem, cmap='gray')\n",
    "        plt.axis('off')  # Turn off axis to remove pixel coordinates\n",
    "        plt.gca().set_position([0, 0, 1, 1])  # Remove all margins\n",
    "        #plt.savefig(\"dem_no_padding.png\", bbox_inches='tight', pad_inches=0)  # Save without padding\n",
    "        plt.show()\n",
    "\n",
    "        # Display Slope\n",
    "        plt.figure(figsize=(10, 10))  # Increase figure size for higher quality\n",
    "        plt.imshow(slope, cmap='gray')\n",
    "        plt.axis('off')  # Turn off axis to remove pixel coordinates\n",
    "        plt.gca().set_position([0, 0, 1, 1])  # Remove all margins\n",
    "        #plt.savefig(\"slope_no_padding.png\", bbox_inches='tight', pad_inches=0)  # Save without padding\n",
    "        plt.show()\n",
    "        #visualize_mask(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71b0314",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_n_complete_samples(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6703667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_n_complete_samples(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
