{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_from_disk\n",
    "# Add the directory containing lit_sam_model.py to the Python path\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "import dataprocessing.rcsHandlingFunctions as rcs\n",
    "from dataprocessing.slope import calculate_slope\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrame from the file \n",
    "df_loaded = pd.read_pickle(\"train_df_sam.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_met = pd.read_pickle(\"df_met.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dem = pd.read_pickle(\"dataframe_avalanches_dem.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_id(path):\n",
    "    # Normalize path separators\n",
    "    path = path.replace(\"\\\\\", \"/\")\n",
    "    # Split on the known folder name (\"avalanche_input/\")\n",
    "    parts = path.split(\"avalanche_input/\")\n",
    "    if len(parts) > 1:\n",
    "        # The next segment in the path should be the id folder\n",
    "        return parts[1].split(\"/\")[0]\n",
    "    return None\n",
    "\n",
    "df_loaded['id'] = df_loaded['image_path'].apply(extract_id)\n",
    "df_met['id'] = df_met['source_file'].apply(extract_id)\n",
    "df_dem['id'] = df_dem['dem_path'].apply(extract_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_loaded.iloc[0][\"id\"])\n",
    "print(df_met.iloc[0][\"id\"])\n",
    "print(df_dem.iloc[0][\"id\"])\n",
    "print(df_loaded.iloc[0][\"image_path\"])\n",
    "print(df_met.iloc[0][\"source_file\"])\n",
    "print(df_dem.iloc[0][\"dem_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_loaded.columns)\n",
    "print(df_met.columns)\n",
    "print(df_dem.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_met into df_loaded using a left join so that only the entries present in df_loaded are kept.\n",
    "merged_df = pd.merge(df_loaded, df_met, on='id', how='left', suffixes=('', '_met'))\n",
    "merged_df = pd.merge(merged_df, df_dem, on='id', how='left', suffixes=('', '_original'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loaded = merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store the indices of non-empty masks\n",
    "valid_indices = [i for i, is_empty in enumerate(df_loaded['empty_mask']) if not is_empty]\n",
    "len(valid_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the image and mask arrays to keep only the non-empty pairs\n",
    "filtered_rcs = df_loaded.loc[valid_indices, 'rcs']\n",
    "filtered_masks = df_loaded.loc[valid_indices, 'mask']\n",
    "filtered_boxes = df_loaded.loc[valid_indices, 'boxes']\n",
    "filtered_DEM = df_loaded.loc[valid_indices, 'dem_original']\n",
    "filtered_met = df_loaded.loc[valid_indices, ['air_temperature_2m', 'precipitation_amount', 'wind_speed_10m', 'relative_humidity_2m', 'air_pressure_at_sea_level']]\n",
    "filtered_id = df_loaded.loc[valid_indices, 'id']\n",
    "print(\"Mask shape:\", filtered_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mask_to_tiff_format(mask):\n",
    "    \"\"\"\n",
    "    Convert the mask to the desired TIFF format in memory and normalize values from 0-255 to 0-1.\n",
    "\n",
    "    Parameters:\n",
    "    mask (numpy.ndarray): The input mask.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The mask in the desired TIFF format with normalized values.\n",
    "    \"\"\"\n",
    "    # Ensure the mask is in the correct format\n",
    "    mask = mask.astype(np.float32)  # Convert to 32-bit float\n",
    "    mask /= 255.0  # Normalize values from 0-255 to 0-1\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to substitute DEM data into the image\n",
    "def substitute_dem(image, dem_data, channel=2):\n",
    "    \"\"\"\n",
    "    Substitute one of the layers of the image with the DEM data.\n",
    "    Args:\n",
    "        image (np.ndarray): The original image.\n",
    "        dem_data (np.ndarray): The DEM data.\n",
    "        channel (int): The channel to be replaced with DEM data.\n",
    "    Returns:\n",
    "        np.ndarray: The modified image with DEM data.\n",
    "    \"\"\"\n",
    "    modified_image = image.copy().astype(np.float32)\n",
    "    modified_image[:, :, channel] = dem_data / 4000.0  # Reduce the DEM data in the 0-1 range\n",
    "    return modified_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_meteo_data(row):\n",
    "    def ensure_list(x):\n",
    "        return x if isinstance(x, list) else [x]\n",
    "        \n",
    "    # Ensure each meteo value is a list\n",
    "    temp = np.array(ensure_list(row[\"air_temperature_2m\"]))\n",
    "    precip = np.array(ensure_list(row[\"precipitation_amount\"]))\n",
    "    wind = np.array(ensure_list(row[\"wind_speed_10m\"]))\n",
    "    humidity = np.array(ensure_list(row[\"relative_humidity_2m\"]))\n",
    "    pressure = np.array(ensure_list(row[\"air_pressure_at_sea_level\"]))\n",
    "    \n",
    "    meteo = np.stack([temp, precip, wind, humidity, pressure], axis=-1)  # shape: (T, 5)\n",
    "    return meteo.tolist()  # consistently return list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process data in batches\n",
    "def process_in_batches(rcs_data, masks, boxes, dems, met, ids, batch_size=100):\n",
    "    for i in range(0, len(rcs_data), batch_size):\n",
    "        batch_rcs_data = rcs_data[i:i + batch_size]\n",
    "        batch_masks = masks[i:i + batch_size]\n",
    "        batch_boxes = boxes[i:i + batch_size]\n",
    "        batch_dems = dems[i:i + batch_size]\n",
    "        batch_met = met[i:i + batch_size]\n",
    "        batch_ids = ids[i:i + batch_size]\n",
    "\n",
    "        # First, merge all rcs instances using _merge_all for each instance.\n",
    "        merged_images = [rcs._merge_all(*rcs_instance)[0] for rcs_instance in batch_rcs_data]\n",
    "        # Each merged image has shape (H, W, 5), where:\n",
    "        #   merged_image[:,:,0]  => HV0 (rescaled hv0)\n",
    "        #   merged_image[:,:,1]  => HV1 (rescaled hv1)\n",
    "        #   merged_image[:,:,2]  => VV0 (rescaled vv0)\n",
    "        #   merged_image[:,:,3]  => VV1 (rescaled vv1)\n",
    "        #   merged_image[:,:,4]  => duplicate of VV1 (can be ignored)\n",
    "\n",
    "        dataset_dict = {\n",
    "            \"VH0\": [cv2.resize(img[:, :, 0], (512, 512), interpolation=cv2.INTER_LINEAR) for img in merged_images],\n",
    "            \"VH1\": [cv2.resize(img[:, :, 1], (512, 512), interpolation=cv2.INTER_LINEAR) for img in merged_images],\n",
    "            \"VV0\": [cv2.resize(img[:, :, 2], (512, 512), interpolation=cv2.INTER_LINEAR) for img in merged_images],\n",
    "            \"VV1\": [cv2.resize(img[:, :, 3], (512, 512), interpolation=cv2.INTER_LINEAR) for img in merged_images],\n",
    "            \"dem\": [cv2.resize(dem / 4000.0, (512, 512), interpolation=cv2.INTER_LINEAR) for dem in batch_dems],\n",
    "            \"label\": [convert_mask_to_tiff_format(np.array(mask)) for mask in batch_masks],\n",
    "            \"box\": [box for box in batch_boxes],\n",
    "            \"met\": [prepare_meteo_data(row) for row in batch_met.to_dict('records')],\n",
    "            \"slope\": [cv2.resize(calculate_slope(dem) / 90, (512, 512), interpolation=cv2.INTER_LINEAR) for dem in batch_dems],\n",
    "            \"id\": batch_ids\n",
    "        }\n",
    "\n",
    "        # Create the dataset using the datasets.Dataset class\n",
    "        dataset = Dataset.from_dict(dataset_dict)\n",
    "        \n",
    "        # Process the dataset (e.g., training, evaluation, etc.)\n",
    "        # Your processing code here\n",
    "        # Save the dataset to disk\n",
    "        dataset.save_to_disk('datasetBoxes' + str(i))\n",
    "        #break;  # Remove this line to process all batches\n",
    "        \n",
    "        # Clear memory\n",
    "        del dataset_dict, dataset\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divide = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_in_batches(filtered_rcs, filtered_masks, filtered_boxes, filtered_DEM, filtered_met, filtered_id, batch_size= filtered_rcs.shape[0]//divide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetsNames = ['datasetBoxes' + str(i) for i in range(0, len(filtered_rcs), filtered_rcs.shape[0]//divide)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datasetsNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_dataset_loaded_and_not_empty(dataset):\n",
    "    \"\"\"\n",
    "    Check if the dataset is loaded and not empty.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (Dataset): The loaded dataset.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the dataset is loaded and not empty, False otherwise.\n",
    "    \"\"\"\n",
    "    if dataset is None:\n",
    "        return False\n",
    "    if len(dataset) == 0:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetList = []\n",
    "for datasetName in datasetsNames:\n",
    "    dataset = load_from_disk(datasetName)\n",
    "    if is_dataset_loaded_and_not_empty(dataset):\n",
    "        print(f\"Dataset '{datasetName}' loaded successfully.\")\n",
    "    datasetList.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datasetList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "# Concatenate the datasets\n",
    "merged_dataset = concatenate_datasets(datasetList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#Check dataset\n",
    "print(merged_dataset.column_names)\n",
    "\n",
    "#draw all the 6 inputs ( slope, dem, VH, VV)\n",
    "plt.figure(figsize=(12, 8))\n",
    "count = 0\n",
    "for i, (key, value) in enumerate(merged_dataset[0].items()):\n",
    "    if key in [\"slope\", \"dem\", \"VH0\", \"VH1\", \"VV0\", \"VV1\"]:\n",
    "        plt.subplot(2, 3, count + 1)\n",
    "        plt.imshow(value, cmap='gray')\n",
    "        plt.title(key)\n",
    "        plt.axis('off')\n",
    "        count += 1\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#slope max and min\n",
    "slope_array = np.array(merged_dataset[0][\"slope\"])\n",
    "print(\"Slope max:\", slope_array.max())\n",
    "print(\"Slope min:\", slope_array.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset to disk\n",
    "merged_dataset.save_to_disk('datasetTrainFinal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from disk\n",
    "dataset = load_from_disk('datasetTrainFinal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and test sets (90% training, 10% test)\n",
    "train_test_split_ratio = 0.9\n",
    "train_dataset, test_dataset = dataset.train_test_split(test_size=1 - train_test_split_ratio, seed = 20).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training dataset into training and validation sets (90% training, 10% validation)\n",
    "train_val_split_ratio = 0.9\n",
    "train_dataset, val_dataset = train_dataset.train_test_split(test_size=1 - train_val_split_ratio, seed = 20).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset.shape)\n",
    "print(val_dataset.shape)\n",
    "print(test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset.save_to_disk('datasetTrainDEMSeparateFloat')\n",
    "val_dataset.save_to_disk('datasetValDEMSeparateFloat')\n",
    "test_dataset.save_to_disk('datasetTestDEMSeparateFloat')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
